{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "iceberg_model.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "JaU4ghciqHd3",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#Statoil Iceberg\n",
        "\n",
        "This notebook demonstrates how to download data using the Kaggle API, augment image data, and build a CNN for the [Statoil/C-CORE Iceberg Classifier Challenge](https://www.kaggle.com/c/statoil-iceberg-classifier-challenge).  \n",
        "\n",
        "\n",
        "\n",
        "##Downloading the Data using the Kaggle API\n",
        "\n",
        "Install the Kaggle library to allow you to use the Kaggle API.  Download your credentials, \"kaggle.json\", from the Kaggle website.  The following instructions to download your kaggle credentials to your local computer:\n",
        "\n",
        "\n",
        "\n",
        "1.   Log in to Kaggle.com.  This should take you to home hub.\n",
        "2.   Click the picture of your profile on the top right corner of your home hub.  Select My Account.\n",
        "3.   In your account settings, find API and click on the Create New API Token button.  This should automatically download your credentials to your computer.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "mej2jtl0hpvy",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#install Kaggle library\n",
        "!pip install Kaggle"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "6gaYT2pCGCN_",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "After running the following code, follow the link to receive a verification code.  Copy the code and paste it in the textbox that appears.  Ensure that the credential is located in the correct folder.  If your credential is placed in your google drive or google cloud storage, ensure that the credential is located in the correct folder.  In the following code below, I need to create a folder in my google drive called \".kaggle\" and place my credential in that folder.\n",
        "\n",
        "```\n",
        "filename = \"/root/.kaggle/kaggle.json\"\n",
        "```\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "rFPRy6n2jE5N",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from googleapiclient.discovery import build\n",
        "import io, os\n",
        "from googleapiclient.http import MediaIoBaseDownload\n",
        "from google.colab import auth\n",
        "auth.authenticate_user()\n",
        "drive_service = build('drive', 'v3')\n",
        "results = drive_service.files().list(\n",
        "        q=\"name = 'kaggle.json'\", fields=\"files(id)\").execute()\n",
        "kaggle_api_key = results.get('files', [])\n",
        "filename = \"/root/.kaggle/kaggle.json\"\n",
        "os.makedirs(os.path.dirname(filename), exist_ok=True)\n",
        "request = drive_service.files().get_media(fileId=kaggle_api_key[0]['id'])\n",
        "fh = io.FileIO(filename, 'wb')\n",
        "downloader = MediaIoBaseDownload(fh, request)\n",
        "done = False\n",
        "while done is False:\n",
        "    status, done = downloader.next_chunk()\n",
        "    print(\"Download %d%%.\" % int(status.progress() * 100))\n",
        "os.chmod(filename, 600)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "fjBw9YoLtJfQ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Download the files from the Statoil/C-Core Iceberg Classifier Challenge using the Kaggle API.  An easy way to get the correct command is to go to the [data page of the competition](https://www.kaggle.com/c/statoil-iceberg-classifier-challenge/data) and copy the command located to the right of the data sources."
      ]
    },
    {
      "metadata": {
        "id": "gPPHQBjBocg6",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!kaggle competitions download -c statoil-iceberg-classifier-challenge"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "nzZVPrvMvs54",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Unzip the downloaded files.  Note that for this Kaggle competition, the data is compressed in the 7z format rather than the Zip format. "
      ]
    },
    {
      "metadata": {
        "id": "9mYWJgZKwDEh",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "os.system( '7z e train.json.7z' )\n",
        "os.system( '7z e test.json.7z' )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "CXcZHWlz9X24",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##Data Preparation\n",
        "\n",
        "The data can be normalized and augmented before being used to train the model.\n"
      ]
    },
    {
      "metadata": {
        "id": "3-BHl7TZm0Lb",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#import python libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "import cv2 "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "f18FfpJKunAT",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "train = pd.read_json(\"train.json\")\n",
        "test = pd.read_json(\"test.json\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "mJ6o_CE7duxO",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Reshape the HH band and HV band for the CNN.  Then normalize the pixel values for the bands."
      ]
    },
    {
      "metadata": {
        "id": "2-Z0hdmA_OmG",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def get_scaled_imgs(df):\n",
        "    imgs = []\n",
        "    \n",
        "    for i, row in df.iterrows():\n",
        "        #make 75x75 image\n",
        "        band_1 = np.array(row['band_1']).reshape(75, 75)\n",
        "        band_2 = np.array(row['band_2']).reshape(75, 75)\n",
        "        \n",
        "        # Rescale\n",
        "        a = (band_1 - band_1.mean()) / (band_1.max() - band_1.min())\n",
        "        b = (band_2 - band_2.mean()) / (band_2.max() - band_2.min())\n",
        "\n",
        "        imgs.append(np.dstack((a, b)))\n",
        "\n",
        "    return np.array(imgs)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "3FKA1QLAAE6x",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "X_train = get_scaled_imgs(train)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ksBZzH8veSat",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Retrieve the labels from the training data."
      ]
    },
    {
      "metadata": {
        "id": "qehn2JfGDVjS",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "Y_train = np.asarray(train['is_iceberg'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "0S53x54DeiJF",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Augment images to increase the size of the dataset.  Each image in the data set is flipped vertically and horizontally."
      ]
    },
    {
      "metadata": {
        "id": "3evvSfNueil5",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def get_more_images(imgs):\n",
        "    \n",
        "    more_images = []\n",
        "    vert_flip_imgs = []\n",
        "    hori_flip_imgs = []\n",
        "      \n",
        "    for i in range(0,imgs.shape[0]):\n",
        "        a=imgs[i,:,:,0]\n",
        "        b=imgs[i,:,:,1]\n",
        "        \n",
        "        av=cv2.flip(a,1)\n",
        "        ah=cv2.flip(a,0)\n",
        "        bv=cv2.flip(b,1)\n",
        "        bh=cv2.flip(b,0)\n",
        "        \n",
        "        vert_flip_imgs.append(np.dstack((av, bv)))\n",
        "        hori_flip_imgs.append(np.dstack((ah, bh)))\n",
        "      \n",
        "    v = np.array(vert_flip_imgs)\n",
        "    h = np.array(hori_flip_imgs)\n",
        "       \n",
        "    more_images = np.concatenate((imgs,v,h))\n",
        "    \n",
        "    return more_images"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "VnFA6jHLf2nA",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "X_train_augmented = get_more_images(X_train) \n",
        "Y_train_augmented = np.concatenate((Y_train,Y_train,Y_train))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "WeMhYRftwkdI",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Separate training data into training and evaluation data."
      ]
    },
    {
      "metadata": {
        "id": "86R7zRJgwjXM",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "X_eval = X_train_augmented[3850:]\n",
        "Y_eval = Y_train_augmented[3850:]\n",
        "X_train_final = X_train_augmented[:3850]\n",
        "Y_train_final = Y_train_augmented[:3850]\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "OV_y5cXsP_gl",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Convert data to be in float32 for better speed when training the model."
      ]
    },
    {
      "metadata": {
        "id": "teCfsogkP-0p",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "X_train_final = X_train_final.astype(np.float32)\n",
        "Y_train_final = Y_train_final.astype(np.float32)\n",
        "X_eval = X_eval.astype(np.float32)\n",
        "Y_eval= Y_eval.astype(np.float32)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "3OjmbUD6g8Jt",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##TensorFlow CNN Model"
      ]
    },
    {
      "metadata": {
        "id": "59u-veUtA4kb",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#reset graph if a previous graph has been created\n",
        "def reset_graph(seed=42):\n",
        "    tf.reset_default_graph()\n",
        "    tf.set_random_seed(seed)\n",
        "    np.random.seed(seed)\n",
        "\n",
        "\n",
        "reset_graph()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "aF0b5tAnR6Qv",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Set the parameters for the CNN."
      ]
    },
    {
      "metadata": {
        "id": "tInwfkXqQ-Vi",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "learning_rate = 0.001\n",
        "n_epochs = 50 \n",
        "num_input = 75*75 \n",
        "num_classes = 2 \n",
        "dropout = 0.2 "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "uhbp5Zb91QKA",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Build the graph.  The graph contains 4 convolution layers, 2 dense layers, and a logit layer."
      ]
    },
    {
      "metadata": {
        "id": "MolxrZIySUtj",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "X = tf.placeholder(tf.float32, shape=(None, 75, 75, 2), name=\"X\")\n",
        "y = tf.placeholder(tf.int64, shape=(None), name=\"y\")\n",
        "\n",
        "\n",
        "with tf.variable_scope('ConvNet'):\n",
        "\n",
        "    he_init = tf.contrib.layers.variance_scaling_initializer()\n",
        "\n",
        "    # Convolution layer #1 - 64 filters and a kernel size of 3\n",
        "    conv1 = tf.layers.conv2d(X, filters=64,  kernel_size=[3, 3], activation=tf.nn.relu)\n",
        "    pool1 = tf.layers.max_pooling2d(conv1, pool_size=[2, 2], strides=2)\n",
        "    dropout1 = tf.layers.dropout(pool1, rate = dropout)\n",
        "    \n",
        "    # Convolution layer #2 - 128 filters and a kernel size of 3 \n",
        "    conv2 = tf.layers.conv2d(dropout1, filters=128,  kernel_size=[3,3], activation=tf.nn.relu)\n",
        "    pool2 = tf.layers.max_pooling2d(conv2, pool_size=[2, 2], strides=2)\n",
        "    dropout2 = tf.layers.dropout(pool2, rate = dropout)\n",
        "\n",
        "    # Convolution layer #3 - 128 filters and a kernel size of 3 \n",
        "    conv3 = tf.layers.conv2d(dropout2, filters=128, kernel_size=[3,3], activation=tf.nn.relu)\n",
        "    pool3 = tf.layers.max_pooling2d(conv3, pool_size=[2, 2], strides=2)\n",
        "    dropout3 = tf.layers.dropout(pool3, rate = dropout)\n",
        "\n",
        "    # Convolution layer #4 - 64 filters and a kernel size of 3 \n",
        "    conv4 = tf.layers.conv2d(dropout3, filters=64, kernel_size=[3,3], activation=tf.nn.relu)\n",
        "    pool4 = tf.layers.max_pooling2d(conv4, pool_size=[2, 2], strides=2)\n",
        "    dropout4 = tf.layers.dropout(pool4, rate = dropout)\n",
        "    \n",
        "    # Flatten the data to a 1-D vector for the fully connected layer\n",
        "    fc1 = tf.contrib.layers.flatten(dropout4)\n",
        "\n",
        "    # Fully connected layer #1\n",
        "    fc2 = tf.layers.dense(fc1, 512, \n",
        "                        kernel_initializer=he_init, activation=tf.nn.relu)\n",
        "    dropout_fc2 = tf.layers.dropout(fc2, rate = dropout)\n",
        "\n",
        "    # Fully connected layer #2\n",
        "    fc3 = tf.layers.dense(dropout_fc2, 256, \n",
        "                        kernel_initializer=he_init, activation=tf.nn.relu)\n",
        "    dropout_fc3 = tf.layers.dropout(fc3, rate = dropout)\n",
        "\n",
        "\n",
        "    logits = tf.layers.dense(dropout_fc3, num_classes, activation=tf.nn.sigmoid)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "fg8M7Wg0SdZL",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "with tf.name_scope(\"loss\"):\n",
        "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)\n",
        "    loss = tf.reduce_mean(xentropy, name=\"loss\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "cllpp_EGSwm4",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "with tf.name_scope(\"train\"):\n",
        "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
        "    training_op = optimizer.minimize(loss)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "NfEZj8xKTK5C",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "with tf.name_scope(\"eval\"):\n",
        "    correct = tf.nn.in_top_k(logits, y, 1)\n",
        "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "UshoTldDTOjg",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "init = tf.global_variables_initializer()\n",
        "saver = tf.train.Saver()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "7NmkwWgVTTdg",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "print('training model\\n')\n",
        "with tf.Session() as sess:\n",
        "    init.run()\n",
        "    for epoch in range(n_epochs):\n",
        "        sess.run(training_op, feed_dict={X: X_train_final, y: Y_train_final})   \n",
        "        acc_train = accuracy.eval(feed_dict={X: X_train_final, y: Y_train_final})\n",
        "        acc_test = accuracy.eval(feed_dict={X: X_eval,\n",
        "                                            y: Y_eval})\n",
        "    \n",
        "        print(epoch, \"Train accuracy:\", acc_train, \"Validation accuracy:\", acc_test)\n",
        "    save_path = saver.save(sess, \"./cam_iceberg_model_final.ckpt\")"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}